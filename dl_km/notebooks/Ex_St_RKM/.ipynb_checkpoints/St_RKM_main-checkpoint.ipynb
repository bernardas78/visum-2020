{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains training and evauation code for St-RKM model on 3Dshapes dataset (https://github.com/deepmind/3d-shapes). Play with the existing code and various tuning-paramteres. As an exercise, inspect the influence of number of gaussian mixtures on the generation quality of the model. \n",
    "\n",
    "### Install and activate python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "print('Successful install.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import skimage\n",
    "import scipy\n",
    "import stiefel_optimizer\n",
    "from skimage.transform import resize\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create directories to save checkpoints and training logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class create_dirs:\n",
    "    \"\"\" Creates directories for logging, Checkpoints and saving trained models \"\"\"\n",
    "\n",
    "    def __init__(self, name, ct):\n",
    "        self.name = name\n",
    "        self.ct = ct\n",
    "        self.dircp = 'checkpoint.pth_{}.tar'.format(self.ct)\n",
    "        self.dirout = '{}_Trained_rkm_{}.tar'.format(self.name, self.ct)\n",
    "\n",
    "    def create(self):\n",
    "        if not os.path.exists('cp/{}'.format(self.name)):\n",
    "            os.makedirs('cp/{}'.format(self.name))\n",
    "\n",
    "        if not os.path.exists('log/{}'.format(self.name)):\n",
    "            os.makedirs('log/{}'.format(self.name))\n",
    "\n",
    "        if not os.path.exists('out/{}'.format(self.name)):\n",
    "            os.makedirs('out/{}'.format(self.name))\n",
    "\n",
    "    def save_checkpoint(self, state, is_best):\n",
    "        if is_best:\n",
    "            torch.save(state, 'cp/{}/{}'.format(self.name, self.dircp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_imshow_format(image):\n",
    "    # convert from CHW to HWC\n",
    "    if image.shape[0] == 1:\n",
    "        return image[0, :, :]\n",
    "    else:\n",
    "        if np.any(np.where(image < 0)):\n",
    "            # first convert back to [0,1] range from [-1,1] range\n",
    "            image = image / 2 + 0.5\n",
    "        return image.transpose(1, 2, 0)\n",
    "    \n",
    "class Resize:\n",
    "    def __init__(self, size):\n",
    "        assert isinstance(size, int) or (isinstance(size, Iterable) and len(size) == 2)\n",
    "        if isinstance(size, int):\n",
    "            self._size = (size, size)\n",
    "        else:\n",
    "            self._size = size\n",
    "\n",
    "    def __call__(self, img: np.ndarray):\n",
    "        resize_image = skimage.transform.resize(img, self._size)\n",
    "        # the resize will return a float32 array\n",
    "        return skimage.util.img_as_float32(resize_image)\n",
    "    \n",
    "class Lin_View(nn.Module):\n",
    "    \"\"\" Unflatten linear layer to be used in Convolution layer\"\"\"\n",
    "    def __init__(self, c, a, b):\n",
    "        super(Lin_View, self).__init__()\n",
    "        self.c, self.a, self.b = c, a, b\n",
    "\n",
    "    def forward(self, x):\n",
    "        try:\n",
    "            return x.view(x.size(0), self.c, self.a, self.b)\n",
    "        except:\n",
    "            return x.view(1, self.c, self.a, self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define encoder ($\\mathbf{\\phi}_{\\theta}$) / decoder ($\\mathbf{\\psi}_{\\zeta}$) networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net1(nn.Module):\n",
    "    \"\"\" Encoder - network architecture \"\"\"\n",
    "    def __init__(self, nChannels, args, cnn_kwargs):\n",
    "        super(Net1, self).__init__()  # inheritance used here.\n",
    "        self.args = args\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(nChannels, self.args.capacity, **cnn_kwargs[0]),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "\n",
    "            nn.Conv2d(self.args.capacity, self.args.capacity * 2, **cnn_kwargs[0]),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "\n",
    "            nn.Conv2d(self.args.capacity * 2, self.args.capacity * 4, **cnn_kwargs[1]),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.args.capacity * 4 * cnn_kwargs[2] ** 2, self.args.x_fdim1),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(self.args.x_fdim1, self.args.x_fdim2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "\n",
    "class Net3(nn.Module):\n",
    "    \"\"\" Decoder - network architecture \"\"\"\n",
    "    def __init__(self, nChannels, args, cnn_kwargs):\n",
    "        super(Net3, self).__init__()\n",
    "        self.args = args\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(self.args.x_fdim2, self.args.x_fdim1),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(self.args.x_fdim1, self.args.capacity * 4 * cnn_kwargs[2] ** 2),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            Lin_View(self.args.capacity * 4, cnn_kwargs[2], cnn_kwargs[2]),  # Unflatten\n",
    "\n",
    "            nn.ConvTranspose2d(self.args.capacity * 4, self.args.capacity * 2, **cnn_kwargs[1]),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "\n",
    "            nn.ConvTranspose2d(self.args.capacity * 2, self.args.capacity, **cnn_kwargs[0]),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "\n",
    "            nn.ConvTranspose2d(self.args.capacity, nChannels, **cnn_kwargs[0]),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Stiefel-RKM Model\n",
    "$\\DeclareMathOperator{\\tr}{\\mathrm{Tr}}$\n",
    "$\\DeclareMathOperator{\\St}{St}$\n",
    "\n",
    "Below we present the Stiefel-Restricted Kernel Machine model as defined in 'Disentangled Representation Learning and Generation with Manifold Optimization' (https://arxiv.org/abs/2006.07046). \n",
    "\n",
    "The objective function is\n",
    "\\begin{equation}\n",
    "\\min_{\\substack{ U\\in \\St(\\ell,m)\\\\\\mathbf{\\theta}, \\mathbf{\\xi}}} \\tr\\left( C_{\\mathbf{\\theta}} - \\mathbb{P}_U C_{\\mathbf{\\theta}} \\mathbb{P}_U\\right) + \\lambda \\frac{1}{n}\\sum_{i=1}^{n}\\left\\{ L_{\\mathbf{\\xi},U}(\\mathbf{x}_i,\\mathbf{\\phi}_{\\mathbf{\\theta}}(\\mathbf{x}_i))\\right\\},\n",
    "\\end{equation}\n",
    "where $U = [\\mathbf{u}_1 \\dots \\mathbf{u}_m]$ is the interconnection matrix belonging to the Stiefel manifold $\\St(\\ell,m)$, that is, the set of $\\ell\\times m$ matrices with orthonormal columns ($\\ell\\geq m$), $\\mathbb{P}_U = U U^\\top$ is the projector and C is the covariance martrix. \n",
    "\n",
    "Here the proposed loss function is\n",
    "\\begin{equation}L^{(\\sigma)}_{\\mathbf{\\xi},U}(\\mathbf{x},\\mathbf{z}) =\\mathbb{E}_{\\mathbf{\\epsilon}\\sim\\mathcal{N}(0,\\mathbb{I}_m)} \\left\\|\\mathbf{x} - \\mathbf{\\psi}_{\\mathbf{\\xi}}\\big(\\mathbb{P}_U\\mathbf{z}+\\sigma  U\\mathbf{\\epsilon}\\big)\\right\\|_2^2, \\end{equation}\n",
    "\n",
    "with $\\mathbf{z} = \\mathbf{\\phi}_{\\mathbf{\\theta}}(\\mathbf{x})$, which is deterministic for $\\sigma = 0$. The noise term $\\sigma U\\mathbf{\\epsilon}$ promotes a _smoother_ decoder network. Another option for the loss used below is the _splitted AE loss_\n",
    "\n",
    "\\begin{equation}\n",
    "L^{(\\sigma),sl}_{\\mathbf{\\xi},U}(\\mathbf{x},\\mathbf{z}) =L^{(0)}_{\\mathbf{\\xi},U}(\\mathbf{x},\\mathbf{z})+\\mathbb{E}_{\\mathbf{\\epsilon}\\sim\\mathcal{N}(0,\\mathbb{I}_m)} \\left\\|\\mathbf{\\psi}_{\\mathbf{\\xi}}\\big(\\mathbb{P}_U\\mathbf{z}\\big) - \\mathbf{\\psi}_{\\mathbf{\\xi}}\\big(\\mathbb{P}_U\\mathbf{z}+\\sigma  U\\mathbf{\\epsilon}\\big)\\right\\|_2^2.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RKM_Stiefel(nn.Module):\n",
    "    \"\"\" Defines the Stiefel RKM model and its loss functions \"\"\"\n",
    "    def __init__(self, ipVec_dim, args, nChannels=1, recon_loss=nn.MSELoss(reduction='sum'), ngpus=1):\n",
    "        super(RKM_Stiefel, self).__init__()\n",
    "        self.ipVec_dim = ipVec_dim\n",
    "        self.ngpus = ngpus\n",
    "        self.args = args\n",
    "        self.nChannels = nChannels\n",
    "        self.recon_loss = recon_loss\n",
    "\n",
    "        # Initialize manifold parameter\n",
    "        self.manifold_param = nn.Parameter(nn.init.orthogonal_(torch.Tensor(self.args.h_dim, self.args.x_fdim2)))\n",
    "\n",
    "        # Settings for Conv layers\n",
    "        self.cnn_kwargs = dict(kernel_size=4, stride=2, padding=1)\n",
    "        if self.ipVec_dim <= 28*28*3:\n",
    "            self.cnn_kwargs = self.cnn_kwargs, dict(kernel_size=3, stride=1), 5\n",
    "        else:\n",
    "            self.cnn_kwargs = self.cnn_kwargs, self.cnn_kwargs, 8\n",
    "\n",
    "        self.encoder = Net1(self.nChannels, self.args, self.cnn_kwargs)\n",
    "        self.decoder = Net3(self.nChannels, self.args, self.cnn_kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        op1 = self.encoder(x)  # features\n",
    "        op1 = op1 - torch.mean(op1, dim=0)  # feature centering\n",
    "        C = torch.mm(op1.t(), op1)  # Covariance matrix\n",
    "\n",
    "        \"\"\" Various types of losses as described in paper \"\"\"\n",
    "        if self.args.loss == 'splitloss':\n",
    "            x_tilde1 = self.decoder(torch.mm(torch.mm(op1, self.manifold_param.t())\n",
    "                                            + self.args.noise_level * torch.randn((x.shape[0], self.args.h_dim)).to(self.args.proc),\n",
    "                                            self.manifold_param))\n",
    "            x_tilde2 = self.decoder(torch.mm(torch.mm(op1, self.manifold_param.t()), self.manifold_param))\n",
    "            f2 = self.args.c_accu * 0.5 * (\n",
    "                    self.recon_loss(x_tilde2.view(-1, self.ipVec_dim), x.view(-1, self.ipVec_dim))\n",
    "                    + self.recon_loss(x_tilde2.view(-1, self.ipVec_dim),\n",
    "                                      x_tilde1.view(-1, self.ipVec_dim))) / x.size(0)  # Recons_loss\n",
    "\n",
    "        elif self.args.loss == 'noisyU':\n",
    "            x_tilde = self.decoder(torch.mm(torch.mm(op1, self.manifold_param.t())\n",
    "                                            + self.args.noise_level * torch.randn((x.shape[0], self.args.h_dim)).to(self.args.proc),\n",
    "                                            self.manifold_param))\n",
    "            f2 = self.args.c_accu * 0.5 * (\n",
    "                self.recon_loss(x_tilde.view(-1, self.ipVec_dim), x.view(-1, self.ipVec_dim))) / x.size(0)  # Recons_loss\n",
    "\n",
    "        elif self.args.loss == 'deterministic':\n",
    "            x_tilde = self.decoder(torch.mm(op1, torch.mm(self.manifold_param.t(), self.manifold_param)))\n",
    "            f2 = self.args.c_accu * 0.5 * (self.recon_loss(x_tilde.view(-1, self.ipVec_dim), x.view(-1, self.ipVec_dim)))/x.size(0)  # Recons_loss\n",
    "\n",
    "        f1 = torch.trace(C - torch.mm(torch.mm(self.manifold_param.t(), self.manifold_param), C))/x.size(0)  # KPCA\n",
    "        return f1 + f2, f1, f2\n",
    "\n",
    "# Accumulate trainable parameters in 2 groups. 1. Manifold_params 2. Network param\n",
    "def param_state(model):\n",
    "    param_g, param_e1 = [], []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and name != 'manifold_param':\n",
    "            param_e1.append(param)\n",
    "        elif name == 'manifold_param':\n",
    "            param_g.append(param)\n",
    "    return param_g, param_e1\n",
    "\n",
    "def stiefel_opti(stief_param, lrg=1e-4):\n",
    "    dict_g = {'params': stief_param, 'lr': lrg, 'momentum': 0.9, 'weight_decay': 0.0005, 'stiefel': True}\n",
    "    return stiefel_optimizer.AdamG([dict_g])  # CayleyAdam\n",
    "\n",
    "def final_compute(model, args, ct, device=torch.device('cuda')):\n",
    "    \"\"\" Utility to re-compute U. Since some datasets could exceed the GPU memory limits, some intermediate\n",
    "    variables are saved  on HDD, and retrieved later\"\"\"\n",
    "    if not os.path.exists('oti/'):\n",
    "        os.makedirs('oti/')\n",
    "\n",
    "    args.shuffle = False\n",
    "    x, _, _ = get_3dshapes_dataloader(args)\n",
    "\n",
    "    # Compute feature-vectors\n",
    "    for i, sample_batch in enumerate(tqdm(x)):\n",
    "        torch.save({'oti': model.encoder(sample_batch[0].to(device))},\n",
    "                   'oti/oti{}_checkpoint.pth_{}.tar'.format(i, ct))\n",
    "\n",
    "    # Load feature-vectors\n",
    "    ot = torch.Tensor([]).to(device)\n",
    "    for i in range(0, len(x)):\n",
    "        ot = torch.cat((ot, torch.load('oti/oti{}_checkpoint.pth_{}.tar'.format(i, ct))['oti']), dim=0)\n",
    "    os.removedirs(\"oti/\")\n",
    "\n",
    "    ot = (ot - torch.mean(ot, dim=0)).to(device)  # Centering\n",
    "    u, _, _ = torch.svd(torch.mm(ot.t(), ot))\n",
    "    u = u[:, :args.h_dim]\n",
    "    with torch.no_grad():\n",
    "        model.manifold_param.masked_scatter_(model.manifold_param != u.t(), u.t())\n",
    "    return torch.mm(ot, u.to(device)), u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_3dshapes_dataloader(args, path_to_data='3dshapes'):\n",
    "    \"\"\"3dshapes dataloader with images rescaled to (28,28,3)\"\"\"\n",
    "    name = '{}/3dshapes.h5'.format(path_to_data)\n",
    "    if not os.path.exists(name):\n",
    "        print('Data at the given path doesn\\'t exist. Downloading now...')\n",
    "        os.system(\"  mkdir 3dshapes;\"\n",
    "                  \"  wget -O 3dshapes/3dshapes.h5 https://storage.googleapis.com/3d-shapes/3dshapes.h5\")\n",
    "        print('Done.')\n",
    "\n",
    "    transform = transforms.Compose([Resize(28), transforms.ToTensor()])\n",
    "    print('Loading data...')\n",
    "    d3shapes_data = d3shapesDataset(name, transform=transform)\n",
    "    d3shapes_loader = DataLoader(d3shapes_data, batch_size=args.mb_size,\n",
    "                                 shuffle=args.shuffle, pin_memory=True, num_workers=args.workers)\n",
    "    _, c, x, y = next(iter(d3shapes_loader))[0].size()\n",
    "    return d3shapes_loader, c*x*y, c\n",
    "\n",
    "\n",
    "class d3shapesDataset(Dataset):\n",
    "    \"\"\"3dshapes dataloader class\"\"\"\n",
    "\n",
    "    lat_names = ('floor_hue', 'wall_hue', 'object_hue', 'scale', 'shape', 'orientation')\n",
    "    lat_sizes = np.array([10, 10, 10, 8, 4, 15])\n",
    "\n",
    "    def __init__(self, path_to_data, subsample=1, transform=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        subsample : int\n",
    "            Only load every |subsample| number of images.\n",
    "        \"\"\"\n",
    "        dataset = h5py.File(path_to_data, 'r')\n",
    "        self.imgs = dataset['images'][::subsample]\n",
    "        self.lat_val = dataset['labels'][::subsample]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.imgs[idx] / 255\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample, self.lat_val[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and hyper-parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Settings ================================\n",
    "parser = argparse.ArgumentParser(description='St-RKM Model', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "parser.add_argument('--dataset_name', type=str, default='3dshapes')\n",
    "parser.add_argument('--h_dim', type=int, default=6, help='Dim of latent vector')\n",
    "parser.add_argument('--capacity', type=int, default=48, help='Conv_filters of network')\n",
    "parser.add_argument('--mb_size', type=int, default=256, help='Mini-batch size')\n",
    "parser.add_argument('--x_fdim1', type=int, default=256, help='Input x_fdim1')\n",
    "parser.add_argument('--x_fdim2', type=int, default=50, help='Input x_fdim2')\n",
    "parser.add_argument('--c_accu', type=float, default=1, help='Input weight on recons_error')\n",
    "parser.add_argument('--noise_level', type=float, default=1e-3, help='Noise-level')\n",
    "parser.add_argument('--loss', type=str, default='deterministic', help='loss type: deterministic/noisyU/splitloss')\n",
    "\n",
    "# Training Settings =============================\n",
    "# Change the device type to the resources on your computer\n",
    "parser.add_argument('--lr', type=float, default=2e-4, help='Input learning rate for ADAM optimizer')\n",
    "parser.add_argument('--lrg', type=float, default=1e-4, help='Input learning rate for Cayley_ADAM optimizer')\n",
    "parser.add_argument('--max_epochs', type=int, default=1, help='Input max_epoch')\n",
    "parser.add_argument('--proc', type=str, default='cuda', help='device type: cuda or cpu')\n",
    "parser.add_argument('--workers', type=int, default=16, help='Number of workers for dataloader')\n",
    "parser.add_argument('--shuffle', type=bool, default=True, help='shuffle dataset: True/False')\n",
    "\n",
    "opt = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(opt.proc)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "ct = time.strftime(\"%Y%m%d-%H%M\")\n",
    "dirs = create_dirs(name=opt.dataset_name, ct=ct)\n",
    "dirs.create()\n",
    "\n",
    "# noinspection PyArgumentList\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(message)s',\n",
    "                    handlers=[logging.FileHandler('log/{}/{}_{}.log'.format(opt.dataset_name, opt.dataset_name, ct)),\n",
    "                              logging.StreamHandler()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and visualize training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, ipVec_dim, nChannels = get_3dshapes_dataloader(args=opt)\n",
    "\n",
    "# Visualize\n",
    "perm1 = torch.randperm(len(xtrain.dataset))\n",
    "it = 0\n",
    "fig, ax = plt.subplots(5, 5)\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        ax[i, j].imshow(convert_to_imshow_format(xtrain.dataset[perm1[it]][0].numpy()))\n",
    "        it+=1\n",
    "plt.suptitle('Ground Truth Data')\n",
    "plt.setp(ax, xticks=[], yticks=[])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngpus = torch.cuda.device_count()\n",
    "\n",
    "rkm = RKM_Stiefel(ipVec_dim=ipVec_dim, args=opt, nChannels=nChannels, ngpus=ngpus).to(device)\n",
    "logging.info(rkm)\n",
    "logging.info(opt)\n",
    "logging.info('\\nN: {}, mb_size: {}'.format(len(xtrain.dataset), opt.mb_size))\n",
    "logging.info('We are using {} GPU(s)!'.format(ngpus))\n",
    "\n",
    "# Accumulate trainable parameters in 2 groups. 1. Manifold_params 2. Network params\n",
    "param_g, param_e1 = param_state(rkm)\n",
    "\n",
    "optimizer1 = stiefel_opti(param_g, opt.lrg)\n",
    "optimizer2 = torch.optim.Adam(param_e1, lr=opt.lr, weight_decay=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Note: Since the training might take longer and could be difficult on laptop, we skip the following 2 cells and go straight to downloading/evaluating pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "Loss_stk = np.empty(shape=[0, 3])\n",
    "cost, l_cost = np.inf, np.inf  # Initialize cost\n",
    "is_best = False\n",
    "t = 1\n",
    "while cost > 1e-10 and t <= opt.max_epochs:  # run epochs until convergence or cut-off\n",
    "    avg_loss, avg_f1, avg_f2 = 0, 0, 0\n",
    "\n",
    "    for _, sample_batched in enumerate(tqdm(xtrain, desc=\"Epoch {}/{}\".format(t, opt.max_epochs))):\n",
    "        loss, f1, f2 = rkm(sample_batched[0].to(device))\n",
    "\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer2.step()\n",
    "        optimizer1.step()\n",
    "\n",
    "        avg_loss += loss.item()\n",
    "        avg_f1 += f1.item()\n",
    "        avg_f2 += f2.item()\n",
    "    cost = avg_loss\n",
    "\n",
    "    # Remember lowest cost and save checkpoint\n",
    "    is_best = cost < l_cost\n",
    "    l_cost = min(cost, l_cost)\n",
    "    dirs.save_checkpoint({\n",
    "        'epochs': t,\n",
    "        'rkm_state_dict': rkm.state_dict(),\n",
    "        'optimizer1': optimizer1.state_dict(),\n",
    "        'optimizer2': optimizer2.state_dict(),\n",
    "        'Loss_stk': Loss_stk,\n",
    "    }, is_best)\n",
    "\n",
    "    logging.info('Epoch {}/{}, Loss: [{}], Kpca: [{}], Recon: [{}]'.format(t, opt.max_epochs, cost, avg_f1, avg_f2))\n",
    "    Loss_stk = np.append(Loss_stk, [[cost, avg_f1, avg_f2]], axis=0)\n",
    "    t += 1\n",
    "\n",
    "logging.info('Finished Training. Lowest cost: {}'\n",
    "             '\\nLoading best checkpoint [{}] & computing sub-space...'.format(l_cost, dirs.dircp))\n",
    "\n",
    "sd_mdl = torch.load('cp/{}/{}'.format(opt.dataset_name, dirs.dircp))\n",
    "rkm.load_state_dict(sd_mdl['rkm_state_dict'])\n",
    "\n",
    "h, U = final_compute(model=rkm, args=opt, ct=ct)\n",
    "logging.info(\"\\nTraining complete in: \" + str(datetime.now() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model and tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'rkm': rkm,\n",
    "            'rkm_state_dict': rkm.state_dict(),\n",
    "            'optimizer1': optimizer1.state_dict(),\n",
    "            'optimizer2': optimizer2.state_dict(),\n",
    "            'Loss_stk': Loss_stk,\n",
    "            'opt': opt,\n",
    "            'h': h, 'U': U}, 'out/{}/{}'.format(opt.dataset_name, dirs.dirout))\n",
    "logging.info('\\nSaved File: {}'.format(dirs.dirout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if not os.path.exists('out/3dshapes/3dshapes_Trained_rkm.tar'):\n",
    "        print('Pre-trained model at given path doesn\\'t exist. Downloading now...')\n",
    "        os.system(\"  wget -O out/3dshapes/3dshapes_Trained_rkm.tar https://www.dropbox.com/s/chwzwaodljq2bn9/3dshapes_Trained_rkm.tar?dl=1\")\n",
    "        print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument('--filename', type=str, default='3dshapes_Trained_rkm', help='Enter Filename')\n",
    "parser.add_argument('--dataset_name', default='3dshapes', type=str, help='Enter dataset name')\n",
    "opt_gen = parser.parse_args(args=[])\n",
    "\n",
    "sd_mdl = torch.load('out/{}/{}.tar'.format(opt_gen.dataset_name, opt_gen.filename),\n",
    "                    map_location=lambda storage, loc: storage)\n",
    "\n",
    "rkm = sd_mdl['rkm']\n",
    "rkm.load_state_dict(sd_mdl['rkm_state_dict'])\n",
    "h = sd_mdl['h']\n",
    "U = sd_mdl['U']\n",
    "opt = sd_mdl['opt']\n",
    "\n",
    "\"\"\" Load Data \"\"\"\n",
    "opt.mb_size = 500\n",
    "opt.workers = 16\n",
    "opt.shuffle = True\n",
    "opt = argparse.Namespace(**vars(opt), **vars(opt_gen))\n",
    "\n",
    "WH = next(iter(xtrain))[0].shape[2]  # Number of channels in image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Visualize correlatedness of latent variables\n",
    "    cov = torch.mm(torch.t(h), h)\n",
    "    print('Cov_mat:\\n {}'.format(cov))\n",
    "    plt.figure()\n",
    "    plt.imshow(cov.detach().cpu().numpy())\n",
    "    plt.title('$Cov(H^{T}H)$')\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize quality of reconstructed samples\n",
    "    perm1 = torch.randperm(len(xtrain.dataset))\n",
    "    m = 5\n",
    "    fig2, ax = plt.subplots(m, m)\n",
    "    it = 0\n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "            ax[i, j].imshow(convert_to_imshow_format(xtrain.dataset[perm1[it]][0].numpy()))\n",
    "            it += 1\n",
    "    plt.suptitle('Ground Truth')\n",
    "    plt.setp(ax, xticks=[], yticks=[])\n",
    "    plt.show()\n",
    "\n",
    "    fig1, ax = plt.subplots(m, m)\n",
    "    x_gen = rkm.decoder(torch.mm(h[perm1[:m * m], :], U.t()).float()).detach().numpy().reshape(-1, nChannels, WH, WH)\n",
    "    it = 0\n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "            ax[i, j].imshow(convert_to_imshow_format(x_gen[it, :, :, :]))\n",
    "            it += 1\n",
    "    plt.suptitle('Reconstructed samples')\n",
    "    plt.setp(ax, xticks=[], yticks=[])\n",
    "    plt.show()\n",
    "    \n",
    "    # Random samples from distribution over H ============================================================\n",
    "    print('Generating random images')\n",
    "    n_components = 1 # Number of components for the GMM\n",
    "    n_samples = 30 # Number of samples for the GMM\n",
    "    \n",
    "    gmm = GMM(n_components=n_components, covariance_type='full').fit(h.detach().cpu().numpy())\n",
    "    z = torch.FloatTensor(gmm.sample(n_samples)[0])\n",
    "\n",
    "    x_gen = rkm.decoder(torch.mm(z, U.t())).detach().cpu().numpy().reshape(-1, nChannels, WH, WH)\n",
    "    \n",
    "    m = 5 # Parameter for plotting\n",
    "    fig1, ax = plt.subplots(m, m)\n",
    "    it = 0\n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "            ax[i, j].imshow(convert_to_imshow_format(x_gen[it, :, :, :]))\n",
    "            it += 1\n",
    "    plt.suptitle('Random generation')\n",
    "    plt.setp(ax, xticks=[], yticks=[])\n",
    "    plt.show()\n",
    "    \n",
    "    m = 5  # Number of steps\n",
    "    fig2, ax = plt.subplots(opt.h_dim, m)\n",
    "\n",
    "    # Interpolation along principal components ================\n",
    "    for i in range(opt.h_dim):\n",
    "        dim = i\n",
    "        mul_off = 0.5  # (for no-offset, set multiplier to 0)\n",
    "\n",
    "        # Manually set the linspace range or get from Unit-Gaussian\n",
    "        lambd = torch.linspace(-2, 2, steps=m)\n",
    "        # lambd = torch.linspace(*utils._get_traversal_range(0.475), steps=m)\n",
    "\n",
    "        uvec = torch.FloatTensor(torch.zeros(h.shape[1]))\n",
    "        uvec[dim] = 1  # unit vector\n",
    "        yoff = mul_off * torch.ones(h.shape[1]).float()\n",
    "        yoff[dim] = 0\n",
    "\n",
    "        yop = yoff.repeat(lambd.size(0), 1) + torch.mm(torch.diag(lambd),\n",
    "                                                       uvec.repeat(lambd.size(0), 1))  # Traversal vectors\n",
    "        x_gen = rkm.decoder(torch.mm(yop, U.t()).float()).detach().numpy().reshape(-1, nChannels, WH, WH)\n",
    "\n",
    "        # Save Images in the directory\n",
    "        if not os.path.exists('Traversal_imgs/{}/{}/{}'.format(opt.dataset_name, opt.filename, dim)):\n",
    "            os.makedirs('Traversal_imgs/{}/{}/{}'.format(opt.dataset_name, opt.filename, dim))\n",
    "\n",
    "        for j in range(x_gen.shape[0]):\n",
    "            scipy.misc.imsave(\n",
    "                'Traversal_imgs/{}/{}/{}/{}im{}.png'.format(opt.dataset_name, opt.filename, dim, dim, j),\n",
    "                convert_to_imshow_format(x_gen[j, :, :, :]))\n",
    "            ax[i, j].imshow(convert_to_imshow_format(x_gen[j, :, :, :]))\n",
    "            \n",
    "    plt.suptitle('Interpolation')\n",
    "    plt.setp(ax, xticks=[], yticks=[])\n",
    "    plt.show()\n",
    "\n",
    "    print('Traversal Images saved in: Traversal_imgs/{}/{}/'.format(opt.dataset_name, opt.filename))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
