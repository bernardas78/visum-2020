{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Adaptation Between Digits\n",
    "\n",
    "#### *RÃ©mi Flamary, Nicolas Courty*\n",
    "\n",
    "In this practical session, we will apply on digit classification the OT based domain adaptation method proposed in \n",
    "\n",
    "N. Courty, R. Flamary, D. Tuia, A. Rakotomamonjy, \"[Optimal transport for domain adaptation](http://remi.flamary.com/biblio/courty2016optimal.pdf)\", Pattern Analysis and Machine Intelligence, IEEE Transactions on, 2016.\n",
    "\n",
    "![otda.png](http://remi.flamary.com/cours/otml/otda.png)\n",
    "\n",
    "To this end, we will try and adapt between the MNIST and USPS datasets. Since those datasets do not have the same resolution (28x28 and 16x16 for MNSIT and USPS) we perform a zeros padding of the USPS digits \n",
    "\n",
    "\n",
    "####  Import modules\n",
    "\n",
    "First, we import the relevant modules. Note that you will need ```sklearn``` to learn the Support Vector Machine classifier and to projet the data with TSNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # always need it\n",
    "import pylab as pl # do the plots\n",
    "\n",
    "# Uncomment the next line to install sklearn with pip\n",
    "# !pip install -U scikit-learn\n",
    "\n",
    "# OR\n",
    "\n",
    "# Uncomment the next line to install sklearn with conda\n",
    "# !conda install scikit-learn\n",
    "\n",
    "# Import sklearn\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Import pot\n",
    "import ot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data and normalization\n",
    "\n",
    "We load the data in memory and perform a normalization of the images so that they all sum to 1.\n",
    "\n",
    "Note that every line in the ```xs``` and ```xt``` is a 28x28 image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.load('data/mnist_usps.npz')\n",
    "\n",
    "xs,ys=data['xs'],data['ys']\n",
    "xt,yt=data['xt'],data['yt']\n",
    "\n",
    "\n",
    "# normalization\n",
    "xs=xs/xs.sum(1,keepdims=True) # every l\n",
    "xt=xt/xt.sum(1,keepdims=True)\n",
    "\n",
    "ns=xs.shape[0]\n",
    "nt=xt.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vizualizing Source (MNIST) and Target (USPS) datasets\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting images\n",
    "def plot_image(x):\n",
    "    pl.imshow(x.reshape((28,28)),cmap='gray')\n",
    "    pl.xticks(())\n",
    "    pl.yticks(())\n",
    "\n",
    "\n",
    "nb=10\n",
    "\n",
    "# Fisrt we plot MNIST\n",
    "pl.figure(1,(nb,nb))\n",
    "for i in range(nb*nb):\n",
    "    pl.subplot(nb,nb,1+i)\n",
    "    c=i%nb\n",
    "    plot_image(xs[np.where(ys==c)[0][i//nb],:])\n",
    "pl.gcf().suptitle(\"MNIST\", fontsize=20);\n",
    "pl.gcf().subplots_adjust(top=0.95)\n",
    "    \n",
    "# Then we plot USPS\n",
    "pl.figure(2,(nb,nb))\n",
    "for i in range(nb*nb):\n",
    "    pl.subplot(nb,nb,1+i)\n",
    "    c=i%nb\n",
    "    plot_image(xt[np.where(yt==c)[0][i//nb],:])\n",
    "pl.gcf().suptitle(\"USPS\", fontsize=20);\n",
    "pl.gcf().subplots_adjust(top=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is a large discrepancy especially between the 1,2 and 5 that have different shapes in both datasets.\n",
    "\n",
    "Also since we have performed zero-padding on the USPS digits they are on average slightly smaller than NMSIT that can take the whole image.\n",
    "\n",
    "\n",
    "### Classification without domain adaptation\n",
    "\n",
    "We learn a classifier on the MNIST dataset (we will not be state of the art on 1000 samples). We evaluate this classifier on MNIST and the USPS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM with reg parameter C=1 and RBF kernel parameter gamma=1e1\n",
    "clf=SVC(C=1,gamma=1e2) # might take time\n",
    "clf.fit(xs,ys)\n",
    "\n",
    "# Compute accuracy\n",
    "ACC_MNIST=clf.score(xs,ys) # beware of overfitting !\n",
    "ACC_USPS=clf.score(xt,yt)\n",
    "\n",
    "print('ACC_MNIST={:1.3f}'.format(ACC_MNIST))\n",
    "print('ACC_USPS={:1.3f}'.format(ACC_USPS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a very large loss in performances. This can be better explained by performing a TSNE embedding on the data.\n",
    "\n",
    "### TSNE of the Source/Target domains\n",
    "\n",
    "[t-distributed stochastic neighbour embedding (TSNE)](http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) is a well-known approach that allows the projection of complex high dimensional data in a lower-dimensional space while keeping its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtot=np.concatenate((xs,xt),axis=0) # all data\n",
    "\n",
    "xp=TSNE().fit_transform(xtot) # this might take a while\n",
    "\n",
    "# separate again; now in 2D\n",
    "xps=xp[:ns,:] \n",
    "xpt=xp[ns:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some plots\n",
    "pl.figure(3,(12,10))\n",
    "\n",
    "pl.scatter(xps[:,0],xps[:,1],c=ys,marker='o',cmap='tab10',label='Source data')\n",
    "pl.scatter(xpt[:,0],xpt[:,1],c=yt,marker='+',cmap='tab10',label='Target data')\n",
    "pl.legend()\n",
    "pl.colorbar()\n",
    "pl.title('TSNE Embedding of the Source/Target data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that while the classes are relatively well clustered, the clusters from source and target dataset rarely overlap. This is the main reason for the important loss in performance between Source and target.\n",
    "\n",
    "### Optimal Transport Domain Adaptation (OTDA)\n",
    "\n",
    "Now we perform domain adaptation with the following 3 steps illustrated at the top of the notebook:\n",
    "\n",
    "1. Compute the OT matrix between source and target datasets\n",
    "1. Perform OT mapping with barycentric mapping (```np.dot```).\n",
    "1. Estimate classifier on the mapped source samples\n",
    "\n",
    "#### 1. OT between domain\n",
    "\n",
    "First, we compute the Cost matrix and visualize it. Note that the samples are sorted by class in both source and target domains to better see the class-based structure in the cost matrix and OT matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code below\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Click or Run the \"...\" to see a possible solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "C=ot.dist(xs,xt)\n",
    "\n",
    "pl.figure(4,(10,10))\n",
    "pl.imshow(C)\n",
    "pl.title('Cost matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the (noisy) structure in the matrix. It is also interesting to note that the class 1 in USPS (second column) is particularly different from all the other classes in MNIST data (even class 1).\n",
    "\n",
    "\n",
    "Next we compute the OT matrix using exact LP OT [ot.emd](http://pot.readthedocs.io/en/stable/all.html#ot.emd) or regularized OT with  [ot.sinkhorn](http://pot.readthedocs.io/en/stable/all.html#ot.sinkhorn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code below\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Click or Run the \"...\" to see a possible solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "G=ot.emd(ot.unif(ns),ot.unif(nt),C)\n",
    "\n",
    "reg=.5e-4\n",
    "# G=ot.sinkhorn(ot.unif(ns),ot.unif(nt),C,reg)\n",
    "\n",
    "pl.figure(5,(10,10))\n",
    "pl.imshow(G,interpolation='bilinear',vmax=G.max()/10)\n",
    "pl.title('OT matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most of the transportation is done in the block-diagonal which means that in average samples from one class are affected by the proper class in the target.\n",
    "\n",
    "#### 2/3 Mapping + Classification\n",
    "\n",
    "Now we perform the barycentric mapping of the samples and training the classifier on the mapped samples. We recommend using a smaller ```gamma=1e1``` here because some samples will be mislabeled and a smooth classifier will work better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code below\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Click or Run the \"...\" to see a possible solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "xst=ns*G.dot(xt)\n",
    "\n",
    "clf=SVC(C=1,gamma=1e1)\n",
    "\n",
    "clf.fit(xst,ys)\n",
    "\n",
    "ACC_USPS2=clf.score(xt,yt)\n",
    "\n",
    "print('ACC_MNIST={:1.3f}'.format(ACC_MNIST))\n",
    "print('ACC_USPS={:1.3f}'.format(ACC_USPS))\n",
    "print('ACC_USPS2={:1.3f}'.format(ACC_USPS2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the adaptation with EMD leads to a performance gain of nearly 10%. You can get even better performances using entropic regularized OT or group lasso regularization.\n",
    "\n",
    "#### TNSE vizualization for OTDA\n",
    "\n",
    "To see the effect of the adaptation we can perform a new TSNE embedding to see if the classes are better aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code below\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Click or Run the \"...\" to see a possible solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "xtot=np.concatenate((xst,xt),axis=0)\n",
    "\n",
    "xp=TSNE().fit_transform(xtot)\n",
    "\n",
    "xps=xp[:ns,:]\n",
    "xpt=xp[ns:,:]\n",
    "\n",
    "\n",
    "pl.figure(6,(12,10))\n",
    "\n",
    "pl.scatter(xps[:,0],xps[:,1],c=ys,marker='o',cmap='tab10',label='Source data')\n",
    "pl.scatter(xpt[:,0],xpt[:,1],c=yt,marker='+',cmap='tab10',label='Target data')\n",
    "pl.legend()\n",
    "pl.colorbar()\n",
    "pl.title('TSNE Embedding of the OT Adapted Source/Target data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that when using emd solver the OT matrix is a permutation where the samples are exactly superimposed. In average the classes are also well transported but there exist many badly transported samples that have a class permutation.\n",
    "\n",
    "\n",
    "#### Transported samples visualization\n",
    "\n",
    "We can now also plot the transported samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code below\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Click or Run the \"...\" to see a possible solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Fisrt we plot MNIST\n",
    "pl.figure(1,(nb,nb))\n",
    "for i in range(nb*nb):\n",
    "    pl.subplot(nb,nb,1+i)\n",
    "    c=i%nb\n",
    "    plot_image(xst[np.where(ys==c)[0][i//nb],:])\n",
    "pl.gcf().suptitle(\"Transported MNIST\", fontsize=20);\n",
    "pl.gcf().subplots_adjust(top=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are the same MNIST samples that have been plotted above but after transportation. Several samples are transported on the wrong class but again on average, the class information is preserved which explain the accuracy gain.\n",
    "\n",
    "### OTDA with regularization\n",
    "\n",
    "We now recommend to try regularized OT and to redo classification/TSNE/Vizu to see the impact of the regularization in term of performances, TNSE and transported samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code below\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
