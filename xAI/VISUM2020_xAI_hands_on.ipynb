{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10-final"
    },
    "colab": {
      "name": "VISUM2020_xAI_hands-on.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc36f_zl20LX",
        "colab_type": "text"
      },
      "source": [
        "# VISUM Explainable AI Hands-on Session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnnMN1aR20La",
        "colab_type": "text"
      },
      "source": [
        "### Learning Objectives "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxPi6NJw20Lb",
        "colab_type": "text"
      },
      "source": [
        "- Learn to train a **classification** model for the task **Fish vs. Non-Fish**\n",
        "- **Explore** and compare **explanations** generated by different **interpretability methods**\n",
        "- Learn how to use **interpretability** saliency maps to do **weakly-supervised object detection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1G7RuMOTxC-",
        "colab_type": "text"
      },
      "source": [
        "### Python packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cCVoPU7PY1Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIlzLqoDXSeG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install innvestigate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJoSMP__g335",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " !git clone --recursive https://github.com/visum-summerschool/visum-2020.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBTwb76MT1ul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ast\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras import losses\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.applications.densenet import DenseNet121\n",
        "from keras.applications.densenet import preprocess_input\n",
        "\n",
        "# import os\n",
        "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHU5x6QL20Lc",
        "colab_type": "text"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrIWftaU20Ld",
        "colab_type": "text"
      },
      "source": [
        "The data used in this hands-on session represents a **subset** of the **training data** used in the **VISUM Project**. Nonetheless, in this dataset, there are also **images without fish**, images containing **one fish**, and others containing **several fish**. \n",
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1kXcMAhYWKogEH7qfJMTfJ2jTgomC8Udq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ezl45AP820Le",
        "colab_type": "text"
      },
      "source": [
        "The **csv file** provided has the following structure: **sequence**, **frame**, **bounding-box** values ([[$x_{min}$, $y_{min}$, $x_{max}$, $y_{max}$]]), and **label**. To tackle the **classification** problem, **bounding-box** labels are **not needed**. However, we will use them later for the evaluation of the saliency maps as pseudo-labels. All the other fields are necessary for this task, with the sequence and frame being the identifiers of the image, and the label the ground-truth annotation used to supervise the learning process. \n",
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=17nSUhC-ggGedEpef_-vwfSvujuTA-MWu)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUbvnQkR20Lf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read data to dataframe\n",
        "# images were resized to 224x224, bounding-box values were converted accordingly\n",
        "df = pd.read_csv('/content/visum-2020/xAI/data.csv') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "Jda8QpsB20Lj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#split data into train, validation and test. \n",
        "#Note: there could be images of the same sequence in different folds \n",
        "\n",
        "test_size=0.01\n",
        "val_size=0.2\n",
        "train_idxs, test_idxs, _, _ = train_test_split(df.index.values, df.label.values, test_size=test_size, stratify=df.label.values, random_state=19)\n",
        "\n",
        "train_df = df.loc[df.index.isin(train_idxs)]\n",
        "test_df = df.loc[df.index.isin(test_idxs)]\n",
        "\n",
        "tr_idxs, val_idxs, _, _ = train_test_split(train_df.index.values, train_df.label.values, test_size=val_size, stratify=train_df.label.values, random_state=19)\n",
        "\n",
        "tr_df = train_df.loc[train_df.index.isin(tr_idxs)]\n",
        "val_df = train_df.loc[train_df.index.isin(val_idxs)]    \n",
        "\n",
        "print('Train data:',len(tr_df), 'images\\nValidation data:', len(val_df), 'images\\nTest data:',len(test_df), 'images')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S93eSSgbVE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load train data\n",
        "train_imgs = [] \n",
        "train_imgs_orig = [] \n",
        "train_labels = [] \n",
        "\n",
        "for seq, frame, clf in zip(tr_df['seq'], tr_df['frame'], tr_df['label']): \n",
        "    path = '/content/visum-2020/xAI/images/seq' + str(seq) + '/img' + str(frame) + '.jpg'\n",
        "    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    train_imgs_orig.append(img)\n",
        "    #preprocess for pre-trained model\n",
        "    img = preprocess_input(img)\n",
        "    \n",
        "    train_imgs.append(img)\n",
        "    train_labels.append(int(clf))\n",
        "\n",
        "train_imgs_orig = np.asarray(train_imgs_orig)                \n",
        "train_imgs = np.asarray(train_imgs)\n",
        "train_labels = np.asarray(train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWn9z4at20Lm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load validation data\n",
        "val_imgs = [] \n",
        "val_imgs_orig = [] \n",
        "val_labels = [] \n",
        "\n",
        "for seq, frame, clf in zip(val_df['seq'], val_df['frame'], val_df['label']): \n",
        "    path = '/content/visum-2020/xAI/images/seq' + str(seq) + '/img' + str(frame) + '.jpg'\n",
        "    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    val_imgs_orig.append(img)\n",
        "    #preprocess for pre-trained model\n",
        "    img = preprocess_input(img)\n",
        "    \n",
        "    val_imgs.append(img)\n",
        "    val_labels.append(int(clf))\n",
        "\n",
        "val_imgs_orig = np.asarray(val_imgs_orig)                \n",
        "val_imgs = np.asarray(val_imgs)\n",
        "val_labels = np.asarray(val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "hezkhvjH20Lp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load test data \n",
        "test_imgs = [] \n",
        "test_imgs_orig = [] \n",
        "test_bbs = []\n",
        "test_labels = [] \n",
        "\n",
        "for seq, frame, bb, clf in zip(test_df['seq'], test_df['frame'], test_df['box_coords'], test_df['label']): \n",
        "    path = '/content/visum-2020/xAI/images/seq' + str(seq) + '/img' + str(frame) + '.jpg'\n",
        "    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    test_imgs_orig.append(img)\n",
        "    \n",
        "    img = preprocess_input(img)\n",
        "    \n",
        "    test_imgs.append(img)\n",
        "    test_labels.append(int(clf))\n",
        "    if(int(clf)==1): \n",
        "        bb = ast.literal_eval(bb)\n",
        "        test_bbs.append(bb)\n",
        "\n",
        "test_imgs_orig = np.asarray(test_imgs_orig)                \n",
        "test_imgs = np.asarray(test_imgs)\n",
        "test_labels = np.asarray(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "pF6UOgVM20Ls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Show test data\n",
        "%matplotlib inline\n",
        "\n",
        "fig = plt.figure(figsize=(28,16))\n",
        "for ii in range(test_imgs_orig.shape[0]): \n",
        "\n",
        "    ax = fig.add_subplot(4, 7, ii+1)\n",
        "    ax.imshow(test_imgs_orig[ii])\n",
        "    if test_labels[ii] == 0:\n",
        "        plt.title('Without Fish')\n",
        "    else:\n",
        "        plt.title('Fish')\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bk48NhG20Lv",
        "colab_type": "text"
      },
      "source": [
        "### Model \n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAt1vK0x20Lv",
        "colab_type": "text"
      },
      "source": [
        "The **classification model** we use in this session is the well-known DenseNet (https://arxiv.org/pdf/1608.06993.pdf), in particular, the **DenseNet-121**. \n",
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1vVAgK7L82dMz8IMu1Cq0Ma6tgSAPmk8H)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZ_e1xXS20Lw",
        "colab_type": "text"
      },
      "source": [
        "A typical strategy used in Computer Vision to fight overfitting and ease the learning process is **transfer learning**. The most common way of doing transfer learning is to **initialize the network** with the **weights** resultant of training the convolutional neural network in the large **ImageNet** dataset (http://www.image-net.org/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "outputPrepend"
        ],
        "id": "0agnSLv420Lx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dense_model = DenseNet121(include_top=True, weights='imagenet', input_shape=(224,224,3))\n",
        "output = Dense(1, activation='sigmoid', name='clf')(dense_model.layers[-2].output)\n",
        "\n",
        "#Then create the corresponding model \n",
        "model = Model(inputs=dense_model.input, outputs=output)\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='adadelta', loss=losses.binary_crossentropy, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sT4Tozlvs8br",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##TRAINING CODE \n",
        "##To save the best performing model in validation\n",
        "#checkpoint = ModelCheckpoint('/content/visum-2020/xAI/model.hdf5', monitor='val_loss', save_best_only=True, save_weights_only=True, verbose=True, mode='min')\n",
        "\n",
        "##Train model\n",
        "#model.fit(train_imgs, train_labels, batch_size=16, epochs=10, verbose=1, validation_data=(val_imgs, val_labels), callbacks=[checkpoint])\n",
        "\n",
        "##TESTING CODE\n",
        "model.load_weights('/content/visum-2020/xAI/model.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "202d9QPE20L3",
        "colab_type": "text"
      },
      "source": [
        "### Interpretability Methods\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-T2A4zL20L3",
        "colab_type": "text"
      },
      "source": [
        "In this section we focus our attention on **interpretability methods** to generate the desired saliency maps to use in order to perform weakly-supervised object detection. For this, we recur to the well-know **iNNvestigate** toolbox (https://github.com/albermax/innvestigate), where we have easy access to several state-of-the-art interpretability methods.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miPlYbmy20L4",
        "colab_type": "text"
      },
      "source": [
        "To provide you with an example, we select the **Deep Taylor** interpretability method (https://www.sciencedirect.com/science/article/pii/S0031320316303582?via%3Dihub) to produce saliency maps for the test data.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1NmzocyO6R-EcRbT0B98ic1mf1tlR-Lst)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZUumkoB20L5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import method from iNNvestigate toolbox\n",
        "from innvestigate.analyzer import create_analyzer\n",
        "analyzer = create_analyzer(\"deep_taylor\", model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "mX9UnL0H20L8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "#Show and save test data saliency maps\n",
        "\n",
        "maps_test = [] \n",
        "\n",
        "for orig, img, clf in zip(test_imgs_orig, test_imgs, test_labels): \n",
        "    ii = 1\n",
        "    img_res = np.reshape(img, (1,img.shape[0],img.shape[1],img.shape[2]))\n",
        "\n",
        "    pred = model.predict(img_res)\n",
        "    pred = np.around(pred, 0)\n",
        "    pred = int(pred)\n",
        "\n",
        "    res = analyzer.analyze(img_res)        \n",
        "    img_int = res.sum(axis=np.argmax(np.asarray(res.shape) == 3))\n",
        "\n",
        "    if np.max(np.abs(img_int)) == 0:\n",
        "      continue\n",
        "    else:\n",
        "      img_int /= np.max(np.abs(img_int))\n",
        "\n",
        "    img_int = np.reshape(img_int, (img.shape[0], img.shape[1]))  \n",
        "    maps_test.append(img_int)\n",
        "    \n",
        "    if(clf==1):\n",
        "        figure, ax = plt.subplots(1, 2, figsize=(10,6))\n",
        "        plt.title('Prediction: {}  Class label: {}'.format(pred, clf), loc='center')\n",
        "        ax[0].imshow(orig)\n",
        "        ax[0].axis('off')\n",
        "        ax[1].imshow(img_int, cmap='hot')\n",
        "        ax[1].axis('off')\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk72K4hX20L_",
        "colab_type": "text"
      },
      "source": [
        "You used Deep Taylor to produce the interpretability saliency maps shown above. Now, **consider other interpretability methods** from the iNNvestigate toolbox and compare them in a qualitative fashion (consider for example: **'gradient'**, **'input_t_gradient'**, **'guided_backprop'**, **'lrp.sequential_preset_a_flat'**). **Which ones** do you consider suitable to be used for **weakly supervised object detection**? Why?\n",
        "\n",
        "\n",
        "Apart from the interpretability method used, **which other factor** has a major **impact** in the **quality of the saliency maps**? Instead of using the trained model, experiment generating the saliency maps produced by the original ImageNet trained DenseNet. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRt-HAyF20MA",
        "colab_type": "text"
      },
      "source": [
        "### Generate bounding-boxes based on saliency maps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izBruSrG20MA",
        "colab_type": "text"
      },
      "source": [
        "- In this section, we generate **bounding-boxes** based on the **saliency maps** obtained before. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "TB6NxEas20MB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "object_preds = [] \n",
        "\n",
        "fig = plt.figure(figsize=(30,25))\n",
        "for ii in range(test_imgs.shape[0]):\n",
        "\n",
        "    img = np.copy(test_imgs_orig[ii])\n",
        "    imap = np.copy(maps_test[ii])\n",
        "\n",
        "    if test_labels[ii] == 1: \n",
        "        box = []\n",
        "        \n",
        "        blur = cv2.GaussianBlur(imap, (7,7), 0) * 255\n",
        "        \n",
        "        ret, ths = cv2.threshold(blur.astype('uint8'), 0, 1, cv2.THRESH_BINARY + cv2.THRESH_OTSU) #Otsu's thresholding after Gaussian filtering\n",
        "        contours, hierarchy = cv2.findContours(ths, cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
        "        \n",
        "        max_cnt = max(contours, key=cv2.contourArea)\n",
        "        for cnt in contours:\n",
        "            cnt_area = cv2.contourArea(cnt)\n",
        "            if cnt_area < 0.1 * cv2.contourArea(max_cnt):\n",
        "                continue\n",
        "            x, y, w, h = cv2.boundingRect(cnt)\n",
        "            img1 = cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,0), 1)\n",
        "            img2 = cv2.rectangle(imap, (x,y), (x+w,y+h), 0.5, 1)\n",
        "            box.append([x,y,x+w,y+h])\n",
        "        object_preds.append(box)\n",
        "\n",
        "    ax = fig.add_subplot(5, 6, ii+1)\n",
        "    ax.imshow(img)\n",
        "    ax.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "AUhcYeQz20ME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Bounding-box predictions:\\n')\n",
        "print(object_preds)\n",
        "print('\\nBounding-box ground-truth:\\n')\n",
        "print(test_bbs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWz5zC4y20MG",
        "colab_type": "text"
      },
      "source": [
        "### Qualitative Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGmU5b-820MG",
        "colab_type": "text"
      },
      "source": [
        "- In this section, we **compare** in a **qualitative way** the weakly-generated bounding boxes with the available ground-truth for the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oy7LUTFK20MH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "j = 0\n",
        "fig = plt.figure(figsize=(30,25))\n",
        "for i in range(test_imgs.shape[0]):\n",
        "    img = np.copy(test_imgs_orig[i])\n",
        "    \n",
        "    if test_labels[i] == 1: \n",
        "        box_pred = object_preds[j]\n",
        "        box_gnd = test_bbs[j]\n",
        "        \n",
        "        for bb in box_pred: # predictions in yellow\n",
        "            x, y, x2, y2 = bb[0], bb[1], bb[2], bb[3]\n",
        "            img1 = cv2.rectangle(img, (x,y), (x2,y2), (0,255,0), 1)\n",
        "\n",
        "        for bb in box_gnd: # ground-truth in blue\n",
        "            x, y, x2, y2 = bb[0], bb[1], bb[2], bb[3]\n",
        "            img1 = cv2.rectangle(img1, (x,y), (x2,y2), (0,0,255), 1)   \n",
        "\n",
        "        j+=1         \n",
        "        \n",
        "    ax = fig.add_subplot(5, 6, i+1)\n",
        "    ax.imshow(img)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6KtMw9Q20MK",
        "colab_type": "text"
      },
      "source": [
        "### Discussion and additional exercise (Semi-supervised Learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kg6ABzgS20MK",
        "colab_type": "text"
      },
      "source": [
        "- In this tutorial we discussed how to use **interpretability saliency maps** to perform object detection when we only have  **weak labels** (class labels) available.\n",
        "\n",
        "- Now, image a scenario where you have class labels for all images but **bounding-box labels only for a percentage of the dataset**. How would you train such a model? \n",
        "\n",
        "- **Suggestion**: in a scenario as the one described, you could first **train a fish classifier** like the one used in this hands-on session, generate the **bounding-box predictions** based on the saliency maps, and, afterwards, use these bounding-box predictions as **pseudo-labels**, training the model as if it was fully supervised. \n",
        "\n",
        "- If you finished this hands-on tutorial and still have time, do the following **additional exercise**: Consider the **baseline model** provided in the **project competition**, adapt it to this dataset, and **train** it when you have **20%** of it **fully annotated**, and **80%** of it with only **weak annotations**, and compare this with the scenarion where you only train with the 20% fully annotated dataset."
      ]
    }
  ]
}